---
editor_options: 
  markdown: 
    wrap: sentence
---

# Chapter 2 Assignment 2, Regression analysis

*Describe the work you have done this week and summarize your learning.*

-   Describe your work and results clearly.
-   Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
-   Assume the reader has no previous knowledge of your data or the more advanced methods you are using.

```{r}
date()
```

## Needed libraries

I like to list all the needed libraries same place in the beginning of my work/code, so the code cell bellow includes libraries that i will be using.

```{r}

#Library for reading in and working with the data
library(tidyverse)

#Libraries for visualization
library(ggplot2)
library(GGally)

#Library for summarysing the data
library(finalfit)

#library for interpreting modelling results
library(broom)
```

## Preparing the data

First step is to read in the data and make sure that everything looks and is correct.
I have set my working directory in IODS-project file (using setwd() -command) so i don't have to make longer specifications for data location.

The analysis is done by using data from "international survey of Approaches of Learning".
Data that will be used in the analysis is subset of original data, because the original data contains variables that will not be used this time.
The subset data includes only 7 variables: gender, age, attitude, deep, stra, surf, points.
Gender is character variable (M (Male), F (Female)), age column includes age in years (number), attitude is global attitude towards statistics (scale to 1-5), deep is mean value of questions about deep learning, stra is mean value of questions about strategic learning, surf is mean value of questions about surface learning, and points is amount of points get in the exam (0 points excluded).

```{r}
#read in the data from data -folder under IODS-project folder
data <- read_csv("data/learning2014.csv")

#check how the data looks by printing first rows
head(data)

#the dimensions of the data
dim(data) 

#structure of the data
str(data)

```

Data looks correct and it includes 166 observations and 7 variables.
Those variables are "gender, age, attitude, deep, stra, surf, points".
Gender is listed as character and all the other variables are numbers.

To get better view of the data, it is good to visualize it.
Visualization is made using ggpairs that creates scatter plot matrix of the variables in the data.
In the ggpairs - function first part is to define data that is used, then mapping argument is used to make visualization more appealing.
In mapping argument, "col" defines how the points are colored (in this case by gender) and "alpha" defines how transparent the points are.
Furthermore the lower argument gives list to the function and the combo part indicates that there are continuous and categorical values.

```{r}

# Create plot using ggpairs that creates scatter plot matrix
visual <- ggpairs(data, mapping = aes(col= gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))

# draw the plot
visual

```

Scatter matrix is now showing distribution of the variables and all the correlations between them.
Few notes from the data:

-   Most of the answers are from females (over 90), but there is still about 50 answers from males as well

-   Most popular age group for both females and males has been between 20-30 years old, but there is few answers from older people as well.

-   Females responses for attitude are more dispersed, while males responses are more focused on certain values.
    Overall there is still answer for every attitude value.

-   Deep learning question answers are weighted towards higher values.

-   the distribution of strategic learning is closer to the normal distribution.

-   Surface learning question answers from Females have a clear top, when the male answers have more left skewed answers.

-   Most of the test takers have scored 20-25 points in the test.

-   Data does not include any high correlations between variables, so there is no need to change variable selection because of that.

-   Highest positive correlation is between Points and Attitude (0.437) and highest negative correlation is between Deep learning questions and Surface learning questions (-0.324)

-   Correlations between variables change depending on the gender and most significant difference is in the deep learning questions and surface learning questions

To get even better view of the data is good also to print out some of the summarys.
First command to use is glimpse() -that will show column names, types and first values in that column.
Second command is missing_glimpse() -command that will reveal if there is missing values.
This part will need the library ´finalfit´

```{r}

#show first rows of each column and the type of the variable
glimpse(data)

#check if there is any missing values
missing_glimpse(data)

#summary of each variable
ff_glimpse(data)

```

Data looks good.
There is no missing values and all the variable types are correct.

## Regression modelling

I chose to make the model using attitude, age and strategic learning variables as explanatory variables.
When using linear model (lm()) it will be created by following structure: lm(depend variable \~ explanatory variable 1 + explanatory variable 2 + ..., data = data that includes all the variables).
After creating the model the summary() -command will show all the important information about the created model.

```{r}

#Create linear model
model <- lm(points ~ attitude + stra + age, data = data)

#show coefficients in a tidy form
tidy(model)

#Print out the model summary
summary(model)

```

To get the wanted results to show more appealing for i will use tidy() -command from the broom library.
In addition to the summary command that will also show important information.

In summary() -output the residual part indicates how the residuals are distributed.
Median value is close to zero, which could indicate that the results could be close to normal distribution, but max and min values indicate that the distribution is slightly off from normal distribution.

All the chosen variables are statistically significant.
Attitude is highly statistically significant, with p-value 4.72e-09.
Stra and age on the other hand have p-value only under 0.1.
Residuals standard error (shown in bottom of the summary() -output) is 5.26 tells how much variation around the predicted values is going to occur.

When interpreting the results based on coefficients few observations can be made:

-   If all the explanatory variables are 0, the points awarded from exam would be 10.89

-   Increase in attitude value will increase received points by 3.4

-   Increase in strategic learning questions value will increase received points by 1

-   Increase in age will lower the received points by 0.09.

Multiple R-squared value for the model is 0.2182 meaning that the model is only explaining 21% of the variation in the points.
Multiple R-squared value is good indicator for how well the model is working and how reliable it is when studying the results.
In this case it would be beneficial to try changing the variables so the model would work better.
(Adjust R-square on the other hand has little bit lower value because its value drops if there is variable in the model that does not help to improve the model in a certain level.)

To study the results even more, it is time to plot them.
Par() -command is used to make all the visuals appear in the same row.
Plot() -command is used to print out the pictures.

```{r}

#define the plotting area, so there is one row and three "columns/plots" in that row. After that use the plot command to plot wanded plots defined by which attribute. 
par(mfrow = c(1,3))
plot(model, which= c(1,2,5))

```

Residuals vs Fitted values -plot shows that residuals form a line that loosely follows the 0 line.
This suggest that the assumption of linear relationship between variables is reasonable.
There is still few outliers that don't follow the overall pattern.
The line could be better, but it does not indicate that the model/ data has significant problems.
If the line is more parabolic then it would indicate that the model need a quadratic term in the explanatory variable, but just a mild curve it is not quite enough to make that kind of change.

QQ-plot (quantile-quantile -plot) shows if the data is conditionally normal.
The closer the points are on the line the closer the data is to normality.
Now it shows that lowest residuals and the highest residuals are not in the line as well as the residuals in the middle.
Overall the residuals are very close to a normal distribution indicating that p-values and confidence intervals that model produced can be trusted.

Residuals vs Leverage plot is created so it is possible to see if there is outliers in the data.
If there is outliers (values that are not approximated well in the model), they have large residual value and it creates large leverage in the model.
The results line is not straight, but turns to a steep descent indicating that there is outliers in the data.
So there could be values that are not modeled correctly, rather, they score too high or too low when modeling the results.
Because of this the model could be better and when studying the results it is important to investigate what kind of situations course the model not work properly.

Overall the model is okay, it is not best to describe the points distribution but it is not wrong ether. Because the model only explanes 21% of the variation in points, and it contains outliers that the residuals vs leverage plot is showing, it would not be useful predicting points. It may work correctly on predicting middle points, but it most likely will be wrong in high/low points. It could work better if the age and stra variables are changed to variables with higher statistical significance. 
